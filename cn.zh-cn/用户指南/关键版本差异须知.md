# 关键版本差异须知<a name="cce_10_0405"></a>

## 1.19升级1.21及以上<a name="section95604451611"></a>

<a name="table134641821155215"></a>
<table><thead align="left"><tr id="row1559615219527"><th class="cellrowborder" valign="top" width="41.94%" id="mcps1.1.3.1.1"><p id="p55964216528"><a name="p55964216528"></a><a name="p55964216528"></a>版本差异</p>
</th>
<th class="cellrowborder" valign="top" width="58.06%" id="mcps1.1.3.1.2"><p id="p5597192113528"><a name="p5597192113528"></a><a name="p5597192113528"></a>建议自检措施</p>
</th>
</tr>
</thead>
<tbody><tr id="row4598122118529"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p10598102117525"><a name="p10598102117525"></a><a name="p10598102117525"></a>Kubernetes 1.21集群版本修复 了exec probe timeouts不生效的BUG，在此修复之前，exec 探测器不考虑 timeoutSeconds 字段。相反，探测将无限期运行，甚至超过其配置的截止日期，直到返回结果。 若用户未配置，默认值为1秒。升级后此字段生效，如果探测时间超过1秒，可能会导致应用健康检查失败并频繁重启。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p559862116521"><a name="p559862116521"></a><a name="p559862116521"></a>升级前检查您使用了exec probe的应用的probe timeouts是否合理。</p>
</td>
</tr>
<tr id="row16550547124"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p197255481211"><a name="p197255481211"></a><a name="p197255481211"></a>CCE的1.19及以上版本的kube-apiserver要求客户侧webhook server的证书必须配置Subject Alternative Names (SAN)字段。否则升级后kube-apiserver调用webhook server失败，容器无法正常启动。</p>
<p id="p177257481626"><a name="p177257481626"></a><a name="p177257481626"></a>根因：Go语言1.15版本废弃了X.509 <a href="https://golang.google.cn/doc/go1.15#commonname" target="_blank" rel="noopener noreferrer">CommonName</a>，CCE的1.19版本的kube-apiserver编译的版本为1.15，若客户的webhook证书没有Subject Alternative Names (SAN)，kube-apiserver不再默认将X509证书的CommonName字段作为hostname处理，最终导致认证失败。CommonName字段作为hostname处理，最终导致认证失败。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p1472534817220"><a name="p1472534817220"></a><a name="p1472534817220"></a>升级前检查您自建webhook server的证书是否配置了SAN字段。</p>
<a name="ul67253481522"></a><a name="ul67253481522"></a><ul id="ul67253481522"><li>若无自建webhook server则不涉及。</li><li>若未配置，建议您配置使用SAN字段指定证书支持的IP及域名。</li></ul>
</td>
</tr>
<tr id="row17886112312259"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p1688702310251"><a name="p1688702310251"></a><a name="p1688702310251"></a>1.21及以上版本不支持管理ARM节点。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p10887202392515"><a name="p10887202392515"></a><a name="p10887202392515"></a>确认升级后无法管理ARM节点不影响您的使用场景。</p>
</td>
</tr>
</tbody>
</table>

## 1.15升级至1.19<a name="section52391740618"></a>

<a name="table13541731724"></a>
<table><thead align="left"><tr id="row4541032022"><th class="cellrowborder" valign="top" width="41.94%" id="mcps1.1.3.1.1"><p id="p11541831925"><a name="p11541831925"></a><a name="p11541831925"></a>版本差异</p>
</th>
<th class="cellrowborder" valign="top" width="58.06%" id="mcps1.1.3.1.2"><p id="p1054832215"><a name="p1054832215"></a><a name="p1054832215"></a>建议自检措施</p>
</th>
</tr>
</thead>
<tbody><tr id="row84456399165"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p58981643131610"><a name="p58981643131610"></a><a name="p58981643131610"></a>CCE 1.19版本的控制面与1.15版本的Kubelet存在兼容性问题。若Master节点升级成功后，节点升级失败或待升级节点发生重启，则节点有极大概率为NotReady状态。</p>
<p id="p3898943151617"><a name="p3898943151617"></a><a name="p3898943151617"></a>主要原因为升级失败的节点有大概率重启kubelet而触发节点注册流程，1.15 kubelet默认注册标签（failure-domain.beta.kubernetes.io/is-baremetal和kubernetes.io/availablezone）被1.19版本kube-apiserver视为非法标签。</p>
<p id="p2898943151614"><a name="p2898943151614"></a><a name="p2898943151614"></a>1.19版本中对应的合法标签为node.kubernetes.io/baremetal和failure-domain.beta.kubernetes.io/zone。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><a name="ol39930523166"></a><a name="ol39930523166"></a><ol id="ol39930523166"><li>正常升级流程不会触发此场景。</li><li>在Master升级完成后尽量避免使用暂停升级功能，快速升级完Node节点。</li><li>若Node节点升级失败且无法修复，请尽快驱逐此节点上的应用，请联系技术支持人员，跳过此节点升级，在整体升级完毕后，重置该节点。</li></ol>
</td>
</tr>
<tr id="row6541237220"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p8541331023"><a name="p8541331023"></a><a name="p8541331023"></a>CCE的1.15版本集群及1.19版本集群将docker的存储驱动文件系统由 xfs切换成ext4，可能会导致升级后的java应用Pod内的import包顺序异常，既而导致Pod异常。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p9541635219"><a name="p9541635219"></a><a name="p9541635219"></a>升级前查看节点上docker配置文件/etc/docker/daemon.json。检查dm.fs配置项是否为xfs。</p>
<a name="ul205412315213"></a><a name="ul205412315213"></a><ul id="ul205412315213"><li>若为ext4或存储驱动为overlay则不涉及。</li><li>若为xfs则建议您在新版本集群预先部署应用，以测试应用与新版本集群是否兼容。</li></ul>
<pre class="screen" id="screen16541331323"><a name="screen16541331323"></a><a name="screen16541331323"></a>{
      "storage-driver": "devicemapper",
      "storage-opts": [
      "dm.thinpooldev=/dev/mapper/vgpaas-thinpool",
      "dm.use_deferred_removal=true",
      <span>"dm.fs=xfs",</span>
      "dm.use_deferred_deletion=true"
      ]
}</pre>
</td>
</tr>
<tr id="row16555312210"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p955833210"><a name="p955833210"></a><a name="p955833210"></a>CCE的1.19及以上版本的kube-apiserver要求客户侧webhook server的证书必须配置Subject Alternative Names (SAN)字段。否则升级后kube-apiserver调用webhook server失败，容器无法正常启动。</p>
<p id="p14552039210"><a name="p14552039210"></a><a name="p14552039210"></a>根因：Go语言1.15版本废弃了X.509 <a href="https://golang.google.cn/doc/go1.15#commonname" target="_blank" rel="noopener noreferrer">CommonName</a>，CCE的1.19版本的kube-apiserver编译的版本为1.15。CommonName字段作为hostname处理，最终导致认证失败。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p9551430215"><a name="p9551430215"></a><a name="p9551430215"></a>升级前检查您自建webhook server的证书是否配置了SAN字段。</p>
<a name="ul4551531725"></a><a name="ul4551531725"></a><ul id="ul4551531725"><li>若无自建webhook server则不涉及。</li><li>若未配置，建议您配置使用SAN字段指定证书支持的IP及域名。</li></ul>
<div class="notice" id="note1879475723113"><a name="note1879475723113"></a><a name="note1879475723113"></a><span class="noticetitle"> 须知： </span><div class="noticebody"><p id="p779495714313"><a name="p779495714313"></a><a name="p779495714313"></a>为减弱此版本差异对集群升级的影响，1.15升级至1.19时，CCE会进行特殊处理，仍然会兼容支持证书不带SAN。但后续升级不再特殊处理，请尽快整改证书，以避免影响后续升级。</p>
</div></div>
</td>
</tr>
<tr id="row7551833218"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p255231525"><a name="p255231525"></a><a name="p255231525"></a>v1.17.17版本及以后的集群CCE自动给用户创建了PSP规则，限制了不安全配置的Pod的创建，如securityContext配置了sysctl的net.core.somaxconn的Pod。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p17551430214"><a name="p17551430214"></a><a name="p17551430214"></a>升级后请参考资料按需开放非安全系统配置，具体请参见<a href="PodSecurityPolicy配置.md">PodSecurityPolicy配置</a>。</p>
</td>
</tr>
</tbody>
</table>

## 1.13升级至1.15<a name="section9267521114"></a>

<a name="table9745885211"></a>
<table><thead align="left"><tr id="row6745188222"><th class="cellrowborder" valign="top" width="41.94%" id="mcps1.1.3.1.1"><p id="p16745188217"><a name="p16745188217"></a><a name="p16745188217"></a>版本差异</p>
</th>
<th class="cellrowborder" valign="top" width="58.06%" id="mcps1.1.3.1.2"><p id="p9745885217"><a name="p9745885217"></a><a name="p9745885217"></a>建议自检措施</p>
</th>
</tr>
</thead>
<tbody><tr id="row10745148624"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p137451589214"><a name="p137451589214"></a><a name="p137451589214"></a>CCE的1.15版本集群及1.19版本集群将docker的存储驱动文件系统由 xfs切换成ext4，可能会导致升级后的java应用Pod内的import包顺序异常，既而导致Pod异常。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p6745783215"><a name="p6745783215"></a><a name="p6745783215"></a>升级前查看节点上docker配置文件/etc/docker/daemon.json。检查dm.fs配置项是否为xfs。</p>
<a name="ul127451582212"></a><a name="ul127451582212"></a><ul id="ul127451582212"><li>若为ext4或存储驱动为overlay则不涉及。</li><li>若为xfs则建议您在新版本集群预先部署应用，以测试应用与新版本集群是否兼容。</li></ul>
<pre class="screen" id="screen167451581527"><a name="screen167451581527"></a><a name="screen167451581527"></a>{
      "storage-driver": "devicemapper",
      "storage-opts": [
      "dm.thinpooldev=/dev/mapper/vgpaas-thinpool",
      "dm.use_deferred_removal=true",
      <span>"dm.fs=xfs",</span>
      "dm.use_deferred_deletion=true"
      ]
}</pre>
</td>
</tr>
<tr id="row14446114063813"><td class="cellrowborder" valign="top" width="41.94%" headers="mcps1.1.3.1.1 "><p id="p1444614400381"><a name="p1444614400381"></a><a name="p1444614400381"></a>vpc集群升级后，由于网络组件的升级，master节点会额外占一个网段。在Master占用了网段后，无可用容器网段时，新建节点无法分配到网段，调度在该节点的pod会无法运行。</p>
</td>
<td class="cellrowborder" valign="top" width="58.06%" headers="mcps1.1.3.1.2 "><p id="p2918181516404"><a name="p2918181516404"></a><a name="p2918181516404"></a>一般集群内节点数量快占满容器网段场景下会出现该问题。例如，容器网段为10.0.0.0/16，可用IP数量为65536，VPC网络IP分配是分配固定大小的网段（使用掩码实现，确定每个节点最多分配多少容器IP），例如上限为128，则此时集群最多支撑65536/128=512个节点，然后去掉Master节点数量为509，此时是1.13集群支持的节点数。集群升级后，在此基础上3台Master节点会各占用1个网段，最终结果就是506台节点。</p>
</td>
</tr>
</tbody>
</table>

